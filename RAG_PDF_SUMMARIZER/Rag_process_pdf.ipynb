{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b6f02e9-4974-4aeb-9da1-c88faf0f0e3f",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) system using LangChain and Ollama. The system processes a PDF document,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "160d1bc4-1f45-4369-888d-ac1e9a7a140c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\konde\\AppData\\Local\\Temp\\ipykernel_25064\\2430642329.py:22: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"llama3.2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the text, several new and improved features are present in YOLOv9 compared to its predecessors:\n",
      "\n",
      "1. **Improved performance**: YOLOv9 achieves state-of-the-art performance on multiple object detection benchmarks, outperforming other popular models such as YOLO MS [7], PP-YOLOE [74], and RT DETR [43].\n",
      "2. **Mosaic data augmentation**: A new mosaic data augmentation strategy is introduced, which combines the benefits of random flipping, color jittering, and rotation to improve model performance.\n",
      "3. **Computational efficiency**: YOLOv9 has a more efficient computational structure than its predecessors, with a lower number of parameters and computations required for inference.\n",
      "4. **Generalized ELAN**: The new ELAN (Efficient Light-weight Anchor Network) module is introduced, which allows for more flexible anchor generation and improves the model's performance on various object detection tasks.\n",
      "5. **SPP-ELAN**: A new SPP (Single Pyramidal Pooling) module is incorporated into YOLOv9, which combines the benefits of pyramid pooling and ELAN to improve the model's ability to capture contextual information.\n",
      "\n",
      "Overall, YOLOv9 represents a significant improvement over its predecessors in terms of performance, efficiency, and flexibility.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Load and split PDF\n",
    "loader = PyPDFLoader(\"yolov9_paper.pdf\")\n",
    "data = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "# Initialize embeddings with Ollama (free and local)\n",
    "embedding_model = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "vectorstore = Chroma.from_documents(docs, embedding=embedding_model)\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "\n",
    "# Initialize LLaMA 3.2 model locally\n",
    "llm = Ollama(model=\"llama3.2\")\n",
    "\n",
    "# Define RAG prompt\n",
    "system_prompt = (\n",
    "    \"You are an AI assistant for question-answering tasks. Use the retrieved context to answer the question. \"\n",
    "    \"If you don't know the answer, say that you don't know. Keep the answer concise and within three sentences.\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "# Create RAG pipeline\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "# Query the model\n",
    "response = rag_chain.invoke({\"input\": \"What is new in YOLOv9?\"})\n",
    "print(response[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa20601-4386-4016-9ab9-58c8cd680bb9",
   "metadata": {},
   "source": [
    "# RAG using google api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5701a68-6a3c-44c7-a1ca-b048135c774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"yolov9_paper.pdf\")\n",
    "data = loader.load()  # entire PDF is loaded as a single Document\n",
    "#data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5e2e7cc-9d35-47c5-99ff-113c3757b439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6725d335-4076-4310-9cb5-f0883b8ca591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents:  96\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# split data\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "\n",
    "print(\"Total number of documents: \",len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f7ef0fa-9c39-4437-9f64-bad8f2b5a9c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-01T01:40:26+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-01T01:40:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'yolov9_paper.pdf', 'total_pages': 18, 'page': 0, 'page_label': '1'}, page_content='GELANâ€™s architecture confirms that PGI has gained su-\\nperior results on lightweight models. We verified the pro-\\nposed GELAN and PGI on MS COCO dataset based ob-\\nject detection. The results show that GELAN only uses\\nconventional convolution operators to achieve better pa-\\nrameter utilization than the state-of-the-art methods devel-\\noped based on depth-wise convolution. PGI can be used\\nfor variety of models from lightweight to large. It can be\\nused to obtain complete information, so that train-from-\\nscratch models can achieve better results than state-of-the-\\nart models pre-trained using large datasets, the compari-\\nson results are shown in Figure 1. The source codes are at:\\nhttps://github.com/WongKinYiu/yolov9.\\n1. Introduction\\nDeep learning-based models have demonstrated far bet-\\nter performance than past artificial intelligence systems in\\nvarious fields, such as computer vision, language process-\\ning, and speech recognition. In recent years, researchers')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "136247e4-31c0-4129-8679-39b5bea6b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import os\n",
    "\n",
    "# Set your Google API Key directly\n",
    "GOOGLE_API_KEY = \"your api key\"\n",
    "\n",
    "# Initialize the embedding model with API key\n",
    "# embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",google_api_key=GOOGLE_API_KEY))\n",
    "\n",
    "# Test embedding\n",
    "# vector = embeddings.embed_query(\"hello, world!\")\n",
    "print(vector[:5])  # Print first 5 values of the embedding vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94f733c-915b-47ff-a2b5-e3f0f0f219c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"What is new in yolov9?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9074b9c1-987f-4f48-9b20-fb975feaacc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70f0612-e3cd-4306-95c6-a9cc13949324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\",temperature=0.3, max_tokens=500)\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "GOOGLE_API_KEY = \"your api key\"\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    temperature=0.3,\n",
    "    max_tokens=500,\n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539a5cff-8897-4413-bb63-820b723f508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aa39eb-6ed0-4cf2-ac4e-948efbff9932",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26761ab7-a6ab-4806-a1fd-9c3da9975a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke({\"input\": \"what is new in YOLOv9?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294e5a79-26c3-453a-87a9-37589edf04ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
