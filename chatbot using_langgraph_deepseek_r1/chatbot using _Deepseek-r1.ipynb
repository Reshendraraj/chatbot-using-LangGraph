{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7608f90-4563-4f59-bbd2-090e181a9012",
   "metadata": {},
   "source": [
    "# A chatbot implementation using LangChain, LangGraph, and DeepSeek R1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b92a87c9-be30-4484-842f-ed9bfddc5dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.3.21\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: c:\\users\\konde\\downloads\\raj_conda\\envs\\env_langchain1\\lib\\site-packages\n",
      "Requires: async-timeout, langchain-core, langchain-text-splitters, langsmith, pydantic, PyYAML, requests, SQLAlchemy\n",
      "Required-by: embedchain, langchain-community\n"
     ]
    }
   ],
   "source": [
    "!pip show langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67505490-1882-40f5-a0f8-5a02cdb335e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langgraph\n",
      "Version: 0.3.18\n",
      "Summary: Building stateful, multi-actor applications with LLMs\n",
      "Home-page: https://www.github.com/langchain-ai/langgraph\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: c:\\users\\konde\\downloads\\raj_conda\\envs\\env_langchain1\\lib\\site-packages\n",
      "Requires: langchain-core, langgraph-checkpoint, langgraph-prebuilt, langgraph-sdk\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70664c46-625a-4fa2-b05a-776b7853370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_community.llms import Ollama     #Correct import\n",
    "\n",
    "#model = OllamaLLM(model=\"llama3.1\")\n",
    "model = OllamaLLM(model=\"deepseek-r1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b373aee3-7c82-42cf-86ab-483970350838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OllamaLLM(model='deepseek-r1')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b54bcb2e-c97a-4afd-9de7-9c77deb66eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List,Dict\n",
    "# from langgraph.graph import StateGraph,START,END\n",
    "# #from langchain_ollama.llms import ollamaLLM       not latest updtaed\n",
    "# from langchain_ollama import Ollama\n",
    "\n",
    "# # import streamlit as st\n",
    "# # from langchain.prompts import ChatPromptTemplate\n",
    "# # from langchain.chains import LLMChain  # Using LLMChain for general LLM usage\n",
    "# # from langchain.llms import Ollama \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc5bdf0e-a2fc-40ff-ba36-ed165afc5666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "user: hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an error occured:name 'straem_graph_updates' is not defined\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854c936c-fe91-40ed-914c-0dec87aea302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de670da1-9148-44e8-ac38-c728d8b8c6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "user:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: <think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Hello! How can I assist you today? ðŸ˜Š\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "user:  give details about qwen llm  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: <think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Qwen-Llama is a large language model developed by the Chinese company Qwen. It is based on the LLaMA (Llama) open-source project and has been fine-tuned with a vast amount of Chinese text data, making it highly proficient in understanding and generating human-like text in Chinese.\n",
      "\n",
      "### Key Features:\n",
      "1. **Language Understanding**: Qwen-Llama excels at comprehending and producing Chinese text, including literary works, news articles, technical documents, and casual conversations.\n",
      "2. **writes like a human**: It can write essays, stories, or other texts with a human-like style, making it useful for creative writing or educational purposes.\n",
      "3. **Translation**: Qwen-Llama can translate between Chinese and other languages, such as English, Japanese, and Spanish, though its translation quality may depend on the specific dataset used for fine-tuning.\n",
      "4. **Conversational AI**: It is designed to engage in conversations, answer questions, and provide information, making it suitable for customer service or chatbot applications.\n",
      "\n",
      "### Capabilities:\n",
      "- **Creative Writing**: Qwen-Llama can generate stories, poems, essays, and other creative content in Chinese.\n",
      "- **Information Retrieval**: It can search through large datasets and provide relevant answers to user queries.\n",
      "- **Translation**: It can translate text from Chinese to other languages or vice versa.\n",
      "\n",
      "### Applications:\n",
      "Qwen-Llama has a wide range of potential applications, including:\n",
      "- **Education**: Assisting students with homework, providing explanations, and offering study materials in Chinese.\n",
      "- **Content Creation**: Helping writers generate ideas, write articles, or create educational content.\n",
      "- **Customer Service**: Providing multilingual support for businesses that operate in China or other Chinese-speaking regions.\n",
      "\n",
      "### Limitations:\n",
      "While Qwen-Llama is a powerful tool, it may struggle with tasks requiring deep domain expertise beyond its training data. Additionally, like all large language models, it can make errors in understanding context, sarcasm, or nuanced language.\n",
      "\n",
      "For more detailed information about Qwen-Llama, you can refer to official documentation or recent research papers published by the developers.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "user:  q\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bye\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_community.llms import Ollama  # Correct import\n",
    "\n",
    "# Define a state\n",
    "class State(TypedDict):\n",
    "    messages: List[Dict[str, str]]\n",
    "\n",
    "# Initialize the StateGraph\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# Initialize the model\n",
    "llm = Ollama(model=\"deepseek-r1\")\n",
    "\n",
    "# Define chatbot function\n",
    "def chatbot(state: State):\n",
    "    user_message = state[\"messages\"][-1][\"content\"]  # Get latest user input\n",
    "    response = llm.invoke(user_message)  # Invoke model with string input\n",
    "    state[\"messages\"].append({\"role\": \"assistant\", \"content\": response})  # Append response\n",
    "    return {\"messages\": state[\"messages\"]}  # Fix key name\n",
    "\n",
    "# Add nodes and edges\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# Stream updates from the graph\n",
    "def stream_graph_updates(user_input: str):\n",
    "    state = {\"messages\": [{\"role\": \"user\", \"content\": user_input}]}\n",
    "    for event in graph.stream(state):\n",
    "        for value in event.values():\n",
    "            print(\"assistant:\", value[\"messages\"][-1][\"content\"])  # Correct key\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"user: \")\n",
    "            if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "                print(\"bye\")\n",
    "                break\n",
    "            stream_graph_updates(user_input)  # Correct function call\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")  # Fix error message\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7c04e5c0-ea93-4c42-9c58-3fbbe9af7a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0+cpu'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb13311-0e48-4173-9695-9395b56cae86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
